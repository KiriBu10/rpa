{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DomesticDeclarations\n",
      "load xes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 10500/10500 [00:02<00:00, 4017.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate NaT...\n",
      "create features...\n",
      "merge dfs\n",
      "save csv...\n",
      "done...\n",
      "--------------------------\n",
      "InternationalDeclarations\n",
      "load xes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 6449/6449 [00:03<00:00, 1726.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate NaT...\n",
      "create features...\n",
      "merge dfs\n",
      "save csv...\n",
      "done...\n",
      "--------------------------\n",
      "Road_Traffic_Fine_Management_Process\n",
      "load xes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 150370/150370 [00:28<00:00, 5255.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate NaT...\n",
      "create features...\n",
      "merge dfs\n",
      "save csv...\n",
      "done...\n",
      "--------------------------\n",
      "PermitLog\n",
      "load xes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 7065/7065 [00:03<00:00, 2110.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate NaT...\n",
      "create features...\n",
      "merge dfs\n",
      "save csv...\n",
      "done...\n",
      "--------------------------\n",
      "PrepaidTravelCost\n",
      "load xes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 2099/2099 [00:00<00:00, 2448.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate NaT...\n",
      "create features...\n",
      "merge dfs\n",
      "save csv...\n",
      "done...\n",
      "--------------------------\n",
      "RequestForPayment\n",
      "load xes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 6886/6886 [00:01<00:00, 3951.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate NaT...\n",
      "create features...\n",
      "merge dfs\n",
      "save csv...\n",
      "done...\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # import dependencies\n",
    "\n",
    "# %%\n",
    "#log section\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "#export log to csv section\n",
    "import pandas as pd\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "import numpy as np\n",
    "#create feature section\n",
    "\n",
    "# %%\n",
    "LIST_DATA_SETS=['DomesticDeclarations','InternationalDeclarations','Road_Traffic_Fine_Management_Process', 'PermitLog', 'PrepaidTravelCost', 'RequestForPayment']\n",
    "\n",
    "\n",
    "for DATASET_NAME in LIST_DATA_SETS:\n",
    "    # %% [markdown]\n",
    "    # # import log\n",
    "\n",
    "    # %%\n",
    "    print(DATASET_NAME)\n",
    "    print('load xes...')\n",
    "    log = xes_importer.apply(f'../../src/datasets/datasets for tagging/{DATASET_NAME}.xes')\n",
    "\n",
    "    # %%\n",
    "    #print(log[0])\n",
    "\n",
    "    # %%\n",
    "    #print(log[0][0])\n",
    "\n",
    "    # %%\n",
    "\n",
    "\n",
    "    # %% [markdown]\n",
    "    # # export log to csv\n",
    "\n",
    "    # %%\n",
    "    df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)\n",
    "    df = df.sort_values(by=['case:concept:name', 'time:timestamp'])\n",
    "\n",
    "\n",
    "    # %%\n",
    "    df\n",
    "\n",
    "    # %%\n",
    "    df['time_delta'] = df['time:timestamp'] - df['time:timestamp'].shift()\n",
    "\n",
    "    # %%\n",
    "    print('calculate NaT...')\n",
    "    from itertools import islice\n",
    "    df=df.reset_index(drop=True)\n",
    "    #first_run=True\n",
    "    #for index, row in df.iterrows():\n",
    "    #    if first_run:\n",
    "    #        first_run=False\n",
    "    #else:\n",
    "        #print(row['case:concept:name'] )\n",
    "        #print(row['time_delta'])\n",
    "        #print(index)\n",
    "    #    if row['case:concept:name'] != df.loc[index-1]['case:concept:name']:\n",
    "    #        df.loc[index,'time_delta'] = pd.NaT\n",
    "\n",
    "\n",
    "    for index, row in islice(df.iterrows(), 1, None):\n",
    "        if row['case:concept:name'] != df.loc[index-1]['case:concept:name']:\n",
    "            df.loc[index,'time_delta'] = pd.NaT\n",
    "\n",
    "    # %%\n",
    "    df\n",
    "\n",
    "    # %%\n",
    "    np.mean(df['time_delta'])\n",
    "\n",
    "    # %%\n",
    "    df.to_pickle(f'../../src/datasets/{DATASET_NAME}_baseline_dataset.pkl')\n",
    "\n",
    "    # %%\n",
    "    df.head()\n",
    "\n",
    "    # %% [markdown]\n",
    "    # # create features\n",
    "    print('create features...')\n",
    "    # %%\n",
    "    df = pd.read_pickle(f'../../src/datasets/{DATASET_NAME}_baseline_dataset.pkl')\n",
    "\n",
    "    # %%\n",
    "    df\n",
    "\n",
    "    # %% [markdown]\n",
    "    # **execution frequency**\n",
    "\n",
    "    # %%\n",
    "    number_of_all_activities = len(df['concept:name'])\n",
    "    number_of_all_activities\n",
    "\n",
    "    # %%\n",
    "    number_of_unique_activities = len(df['concept:name'].value_counts().keys())\n",
    "    number_of_unique_activities\n",
    "\n",
    "    # %%\n",
    "    df_execution_frequency = pd.DataFrame({'concept:name':df['concept:name'].value_counts().keys(),'execution_frequncy': df['concept:name'].value_counts().values / number_of_all_activities})\n",
    "    df_execution_frequency\n",
    "\n",
    "    # %% [markdown]\n",
    "    # **execution time**\n",
    "\n",
    "    # %%\n",
    "    # median\n",
    "    series_execution_time_median = df.groupby(['concept:name'])['time_delta'].median()\n",
    "    df_execution_time_median = pd.DataFrame({'concept:name':series_execution_time_median.keys(),'execution_time_median': series_execution_time_median.values})\n",
    "    df_execution_time_median\n",
    "\n",
    "    # %%\n",
    "    #mean\n",
    "    series_execution_time_mean = df.groupby(['concept:name'])['time_delta'].mean()\n",
    "    df_execution_time_mean = pd.DataFrame({'concept:name':series_execution_time_mean.keys(),'execution_time_mean': series_execution_time_mean.values})\n",
    "    df_execution_time_mean\n",
    "\n",
    "    # %%\n",
    "    #std\n",
    "    df_copy = df.copy()\n",
    "    df_copy['time_delta'] = df_copy['time_delta'].apply(lambda x:x.days)\n",
    "\n",
    "    series_execution_time_std = df_copy.groupby(['concept:name'])['time_delta'].std()\n",
    "    df_execution_time_std = pd.DataFrame({'concept:name':series_execution_time_std.keys(),'execution_time_std_days': series_execution_time_std.values})\n",
    "    df_execution_time_std\n",
    "\n",
    "    # %%\n",
    "    # median\n",
    "    series_execution_time_skew = df_copy.groupby(['concept:name'])['time_delta'].skew()\n",
    "    df_execution_time_skew = pd.DataFrame({'concept:name':series_execution_time_skew.keys(),'execution_time_skew': series_execution_time_skew.values})\n",
    "    df_execution_time_skew\n",
    "\n",
    "    # %% [markdown]\n",
    "    # **failure rate**\n",
    "\n",
    "    # %%\n",
    "    series_value_counts_activities = df['concept:name'].value_counts()\n",
    "    series_value_counts_activities\n",
    "\n",
    "    # %%\n",
    "    list_failure_rate_absolute=[]\n",
    "    for key in series_value_counts_activities.keys():\n",
    "        number_of_traces=0\n",
    "        for activities in df.groupby('case:concept:name')['concept:name'].sum().values:\n",
    "            if key in activities:\n",
    "                number_of_traces=number_of_traces+1\n",
    "        list_failure_rate_absolute.append(number_of_traces)\n",
    "\n",
    "    list_failure_rate = series_value_counts_activities.values / list_failure_rate_absolute\n",
    "\n",
    "    df_failure_rate = pd.DataFrame({'concept:name':series_value_counts_activities.keys(),'failure_rate': list_failure_rate})\n",
    "    df_failure_rate\n",
    "\n",
    "    # %% [markdown]\n",
    "    # **number of ressources**\n",
    "\n",
    "    # %%\n",
    "    series_number_of_resources_per_activity = df.groupby(['concept:name'])['org:resource'].nunique()\n",
    "    df_number_of_resources = pd.DataFrame({'concept:name':series_number_of_resources_per_activity.keys(),'number_of_resources': series_number_of_resources_per_activity.values})\n",
    "    df_number_of_resources\n",
    "\n",
    "\n",
    "    # %%\n",
    "\n",
    "\n",
    "    # %% [markdown]\n",
    "    # **task maturity**\n",
    "\n",
    "    # %%\n",
    "    list_activities = df['concept:name'].unique()\n",
    "\n",
    "    # %%\n",
    "    #deterministic preceding\n",
    "    a=[activities[0] for activities in list(set(zip(df['concept:name'][1:],df['concept:name'][:-1])))]\n",
    "    list_has_same_preceding=[]\n",
    "    for activity in list_activities:\n",
    "        if a.count(activity) ==1:\n",
    "            list_has_same_preceding.append(True)\n",
    "        else:\n",
    "            list_has_same_preceding.append(False)\n",
    "\n",
    "    # %%\n",
    "    df_deterministic_p = pd.DataFrame({'concept:name':list_activities,'deterministic_p': list_has_same_preceding})\n",
    "    df_deterministic_p\n",
    "\n",
    "    # %%\n",
    "    #deterministic following\n",
    "    a=[activities[0] for activities in list(set(zip(df['concept:name'][:-1],df['concept:name'][1:])))]\n",
    "    list_has_same_follower=[]\n",
    "    for activity in list_activities:\n",
    "        if a.count(activity) ==1:\n",
    "            list_has_same_follower.append(True)\n",
    "        else:\n",
    "            list_has_same_follower.append(False)\n",
    "\n",
    "    # %%\n",
    "    df_deterministic_f = pd.DataFrame({'concept:name':list_activities,'deterministic_f': list_has_same_follower})\n",
    "    df_deterministic_f\n",
    "\n",
    "    # %% [markdown]\n",
    "    # **standardization**\n",
    "\n",
    "    # %%\n",
    "    #standardization_p\n",
    "    a=[activities[0] for activities in list(set(zip(df['concept:name'][1:],df['concept:name'][:-1])))]\n",
    "    list_standardization_p=[]\n",
    "    for activity in list_activities:\n",
    "        list_standardization_p.append(a.count(activity))\n",
    "\n",
    "    df_standardization_p = pd.DataFrame({'concept:name':list_activities,'standardization_p': list_standardization_p})\n",
    "    df_standardization_p\n",
    "\n",
    "    # %%\n",
    "    #standardization_f\n",
    "    a=[activities[0] for activities in list(set(zip(df['concept:name'][:-1],df['concept:name'][1:])))]\n",
    "    list_standardization_f=[]\n",
    "    for activity in list_activities:\n",
    "        list_standardization_f.append(a.count(activity))\n",
    "\n",
    "    df_standardization_f = pd.DataFrame({'concept:name':list_activities,'standardization_f': list_standardization_f})\n",
    "    df_standardization_f\n",
    "\n",
    "    # %%\n",
    "\n",
    "\n",
    "    # %%\n",
    "\n",
    "\n",
    "    # %% [markdown]\n",
    "    # # merge dfs\n",
    "\n",
    "    # %% [markdown]\n",
    "    # df_standardization_p\n",
    "    # df_standardization_f df_execution_time_std\n",
    "    # df_execution_frequency df_execution_time_median df_execution_time_mean df_failure_rate df_number_of_resources df_deterministic_p df_deterministic_f \n",
    "\n",
    "    # %%\n",
    "    print('merge dfs')\n",
    "    final_dataset = df_execution_frequency.merge(df_execution_time_median, on='concept:name',how='inner')\n",
    "    final_dataset = final_dataset.merge(df_execution_time_mean, on='concept:name',how='inner')\n",
    "    final_dataset = final_dataset.merge(df_execution_time_std, on='concept:name',how='inner')\n",
    "    final_dataset = final_dataset.merge(df_execution_time_skew, on='concept:name',how='inner')\n",
    "    final_dataset = final_dataset.merge(df_failure_rate, on='concept:name',how='inner')\n",
    "    final_dataset = final_dataset.merge(df_number_of_resources, on='concept:name',how='inner')\n",
    "    final_dataset = final_dataset.merge(df_deterministic_p, on='concept:name',how='inner')\n",
    "    final_dataset = final_dataset.merge(df_deterministic_f, on='concept:name',how='inner')\n",
    "    final_dataset = final_dataset.merge(df_standardization_p, on='concept:name',how='inner')\n",
    "    final_dataset = final_dataset.merge(df_standardization_f, on='concept:name',how='inner')\n",
    "    final_dataset['dataset_name']=DATASET_NAME\n",
    "    final_dataset\n",
    "    print('save csv...')\n",
    "    final_dataset.to_csv(f'../../src/datasets/{DATASET_NAME}.csv')\n",
    "    print('done...')\n",
    "    print('--------------------------')\n",
    "    # %%\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82f297eb6bc397f0ad5492d2b9b5b7d0cc8237fc52500bf14e4e2453ad84b23c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit ('rpa_mac': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
