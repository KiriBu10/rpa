{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "      <th>object:name</th>\n",
       "      <th>object:instance</th>\n",
       "      <th>object:status</th>\n",
       "      <th>action:name</th>\n",
       "      <th>action:status</th>\n",
       "      <th>org:actor:name</th>\n",
       "      <th>org:actor:instance</th>\n",
       "      <th>org:passive:name</th>\n",
       "      <th>org:passive:instance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>accept order</td>\n",
       "      <td>['Automated activity', 'Physical activity', 'U...</td>\n",
       "      <td>order</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accept</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>acquire equipment</td>\n",
       "      <td>['Physical activity', 'User activity']</td>\n",
       "      <td>equipment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>acquire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>advertise post</td>\n",
       "      <td>['Physical activity', 'User activity']</td>\n",
       "      <td>post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>advertise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>alert supervisor</td>\n",
       "      <td>['Automated activity', 'Physical activity', 'U...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>supervisor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>analyze defect</td>\n",
       "      <td>['Physical activity', 'User activity']</td>\n",
       "      <td>defect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>analyze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               data  \\\n",
       "0           0       accept order   \n",
       "1           1  acquire equipment   \n",
       "2           2     advertise post   \n",
       "3           3   alert supervisor   \n",
       "4           4     analyze defect   \n",
       "\n",
       "                                               label object:name  \\\n",
       "0  ['Automated activity', 'Physical activity', 'U...       order   \n",
       "1             ['Physical activity', 'User activity']   equipment   \n",
       "2             ['Physical activity', 'User activity']        post   \n",
       "3  ['Automated activity', 'Physical activity', 'U...         NaN   \n",
       "4             ['Physical activity', 'User activity']      defect   \n",
       "\n",
       "   object:instance  object:status action:name  action:status  org:actor:name  \\\n",
       "0              NaN            NaN      accept            NaN             NaN   \n",
       "1              NaN            NaN     acquire            NaN             NaN   \n",
       "2              NaN            NaN   advertise            NaN             NaN   \n",
       "3              NaN            NaN       alert            NaN             NaN   \n",
       "4              NaN            NaN     analyze            NaN             NaN   \n",
       "\n",
       "   org:actor:instance org:passive:name  org:passive:instance  \n",
       "0                 NaN              NaN                   NaN  \n",
       "1                 NaN              NaN                   NaN  \n",
       "2                 NaN              NaN                   NaN  \n",
       "3                 NaN       supervisor                   NaN  \n",
       "4                 NaN              NaN                   NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = f'../../src/datasets/01_basic_dataset_for_classification.csv'\n",
    "df = pd.read_csv(datapath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accept order</td>\n",
       "      <td>['Automated activity', 'Physical activity', 'U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acquire equipment</td>\n",
       "      <td>['Physical activity', 'User activity']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>advertise post</td>\n",
       "      <td>['Physical activity', 'User activity']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alert supervisor</td>\n",
       "      <td>['Automated activity', 'Physical activity', 'U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>analyze defect</td>\n",
       "      <td>['Physical activity', 'User activity']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text                                             labels\n",
       "0       accept order  ['Automated activity', 'Physical activity', 'U...\n",
       "1  acquire equipment             ['Physical activity', 'User activity']\n",
       "2     advertise post             ['Physical activity', 'User activity']\n",
       "3   alert supervisor  ['Automated activity', 'Physical activity', 'U...\n",
       "4     analyze defect             ['Physical activity', 'User activity']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[['data', 'label']]\n",
    "df=df.rename(columns={'data':'text', 'label':'labels'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='labels'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby(['labels']).size().plot.bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Automated activity', 'Physical activity', 'User activity']\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Physical activity', 'User activity']                          231\n",
       "['Automated activity', 'Physical activity', 'User activity']     58\n",
       "['Physical activity']                                            25\n",
       "['User activity']                                                12\n",
       "['Automated activity', 'User activity']                          11\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')#(\"vinai/bertweet-base\")#('bert-base-cased')\n",
    "labels = {\"['Physical activity', 'User activity']\":0,\n",
    "          \"['Automated activity', 'Physical activity', 'User activity']\":0,\n",
    "          \"['Physical activity']\":1,\n",
    "          \"['User activity']\":2,\n",
    "          \"['Automated activity', 'User activity']\":0\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class']=df.labels.apply(lambda x: labels[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    300\n",
       "1     25\n",
       "2     12\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8902077151335311"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(300/sum(df['class'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1233333333333333"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['class'].value_counts())/300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.48"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['class'].value_counts())/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.083333333333332"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['class'].value_counts())/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07418397626112759"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(25/sum(df['class'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03560830860534125"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(12/sum(df['class'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = [labels[label] for label in df['labels']]\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 130, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['text']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('vinai/bertweet-base')#(\"vinai/bertweet-base\")'bert-base-cased'\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.num_labels=3\n",
    "        #self.weight=nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_label=train_label.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor([28,13,1,4,3]))\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    \n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                \n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                train_label=train_label.type(torch.LongTensor)\n",
    "                train_label = train_label.to(device)\n",
    "                batch_loss = criterion(output, train_label)\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    val_label=val_label.type(torch.LongTensor)\n",
    "                    val_label = val_label.to(device)\n",
    "                    batch_loss = criterion(output, val_label)\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} | Train Accuracy: {total_acc_train / len(train_data): .3f} | Val Loss: {total_loss_val / len(val_data): .3f} | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "              test_label = test_label.to(device)\n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "              #print(output.argmax(dim=1))\n",
    "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "              total_acc_test += acc\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluate(model, test_data):\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "    #print(test_dataloader)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "            \n",
    "            \n",
    "            outputs = model(input_id, mask)\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            #probs = F.Softmax(outputs, dim=1)\n",
    "            \n",
    "            predictions.extend(preds)\n",
    "            #prediction_probs.extend(probs)\n",
    "            real_values.extend(test_label)\n",
    "            \n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    #prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return predictions, prediction_probs, real_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 101 135\n"
     ]
    }
   ],
   "source": [
    "#np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                     [int(.3*len(df)), int(.6*len(df))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing BertModel: ['roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'lm_head.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'lm_head.decoder.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'lm_head.dense.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'lm_head.decoder.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.10.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.8.intermediate.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 51/51 [00:04<00:00, 11.97it/s]\n",
      "  4%|▍         | 2/51 [00:00<00:03, 16.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.440 | Train Accuracy:  0.782 | Val Loss:  0.180 | Val Accuracy:  0.901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:04<00:00, 12.36it/s]\n",
      "  4%|▍         | 2/51 [00:00<00:02, 16.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.167 | Train Accuracy:  0.881 | Val Loss:  0.148 | Val Accuracy:  0.901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:04<00:00, 12.49it/s]\n",
      "  4%|▍         | 2/51 [00:00<00:02, 16.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.157 | Train Accuracy:  0.881 | Val Loss:  0.143 | Val Accuracy:  0.901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:04<00:00, 12.71it/s]\n",
      "  4%|▍         | 2/51 [00:00<00:03, 15.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.178 | Train Accuracy:  0.881 | Val Loss:  0.144 | Val Accuracy:  0.901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:03<00:00, 12.82it/s]\n",
      "  4%|▍         | 2/51 [00:00<00:02, 17.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.165 | Train Accuracy:  0.881 | Val Loss:  0.138 | Val Accuracy:  0.901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:04<00:00, 12.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.145 | Train Accuracy:  0.881 | Val Loss:  0.129 | Val Accuracy:  0.901\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 6\n",
    "model = BertClassifier()\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.889\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_pred_probs, y_test = get_evaluate(\n",
    "  model,\n",
    "  df_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94       120\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.89       135\n",
      "   macro avg       0.30      0.33      0.31       135\n",
      "weighted avg       0.79      0.89      0.84       135\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kiran.busch\\OneDrive - Kühne Logistics University\\project collab mannheim\\git\\env\\rpa\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\kiran.busch\\OneDrive - Kühne Logistics University\\project collab mannheim\\git\\env\\rpa\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\kiran.busch\\OneDrive - Kühne Logistics University\\project collab mannheim\\git\\env\\rpa\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  \n",
    "  review_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "\n",
    "      texts = d[\"review_text\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "      review_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(probs)\n",
    "      real_values.extend(targets)\n",
    "\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=5, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89893cb23d3dcc86259f378cdfae0017b55a19df352dd69d184048d73a3ee7ed"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('rpa': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
